{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun Phrase Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A chatbot (also known as a talkbot, chatterbot, Bot, IM bot, interactive agent, or Artificial Conversational Entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods. Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the Turing test. Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition. Some chatterbots use sophisticated natural language processing systems, but many simpler systems scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.\\nThe term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs. Today, most chatbots are either accessed via virtual assistants such as Google Assistant and Amazon Alexa, via messaging apps such as Facebook Messenger or WeChat, or via individual organizations\\' apps and websites. Chatbots can be classified into usage categories such as conversational commerce (e-commerce via chat), analytics, communication, customer support, design, developer tools, education, entertainment, finance, food, games, health, HR, marketing, news, personal, productivity, shopping, social, sports, travel and utilities.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('chatbot.txt','r',errors = 'ignore')\n",
    "raw = f.read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chatbot\n",
      "a talkbot\n",
      "chatterbot\n",
      "Bot\n",
      "IM bot\n",
      "interactive agent\n",
      "Artificial Conversational Entity\n",
      "a computer program\n",
      "an artificial intelligence\n",
      "which\n",
      "a conversation\n",
      "auditory or textual methods\n",
      "Such programs\n",
      "a human\n",
      "a conversational partner\n",
      "the Turing test\n",
      "Chatbots\n",
      "dialog systems\n",
      "various practical purposes\n",
      "customer service\n",
      "information acquisition\n",
      "Some chatterbots\n",
      "sophisticated natural language processing systems\n",
      "many simpler systems\n",
      "keywords\n",
      "the input\n",
      "a reply\n",
      "the most matching keywords\n",
      "the most similar wording pattern\n",
      "a database\n",
      "The term\n",
      "\"ChatterBot\n",
      "Michael Mauldin\n",
      "creator\n",
      "the first Verbot, Julia\n",
      "these conversational programs\n",
      "most chatbots\n",
      "virtual assistants\n",
      "Google Assistant\n",
      "Amazon Alexa\n",
      "apps\n",
      "Facebook Messenger\n",
      "WeChat\n",
      "individual organizations' apps\n",
      "websites\n",
      "Chatbots\n",
      "usage categories\n",
      "conversational commerce\n",
      "e\n",
      "commerce\n",
      "analytics\n",
      "communication\n",
      "customer support\n",
      "design\n",
      "developer tools\n",
      "education\n",
      "entertainment\n",
      "finance\n",
      "food\n",
      "games\n",
      "health\n",
      "HR\n",
      "marketing\n",
      "news\n",
      "productivity\n",
      "shopping\n",
      "sports\n",
      "travel\n",
      "utilities\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(raw)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking with NLTK\n",
    "\n",
    "The method for chunking text with NLTK is slightly different than with Spacy.  It's more complicated, but ultimatley more flexible.  It uses the combination of POS tagging and `RegEx` to parse groups of words for patterns.  Because of the use of `RegEx`, it's possible to create any sets of POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_chunker(sent, expression):\n",
    "\n",
    "    sent = pos_tag(word_tokenize(sent))\n",
    "    cp = nltk.RegexpParser(expression)\n",
    "    chunked = cp.parse(sent)\n",
    "\n",
    "    for chunk in chunked.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Autonomous/JJ cars/NNS)\n"
     ]
    }
   ],
   "source": [
    "sent = \"Autonomous cars shift insurance liability toward manufacturers\"\n",
    "# Chunk 0: Noun followed by Noun\n",
    "reg_chunker(sent, r'NP: {<JJ>+<NN.?>}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP IM/NNP bot/NN)\n",
      "(NP Artificial/NNP Conversational/NNP Entity/NNP)\n",
      "(NP computer/NN program/NN)\n",
      "(NP Turing/NNP test/NN)\n",
      "(NP dialog/NN systems/NNS)\n",
      "(NP customer/NN service/NN)\n",
      "(NP information/NN acquisition/NN)\n",
      "(NP language/NN processing/NN systems/NNS)\n",
      "(NP simpler/NN systems/NNS)\n",
      "(NP wording/NN pattern/NN)\n",
      "(NP Michael/NNP Mauldin/NNP)\n",
      "(NP Google/NNP Assistant/NNP)\n",
      "(NP Amazon/NNP Alexa/NNP)\n",
      "(NP Facebook/NNP Messenger/NNP)\n",
      "(NP customer/NN support/NN)\n",
      "(NP developer/NN tools/NNS)\n"
     ]
    }
   ],
   "source": [
    "# Chunk 1: Noun followed by Noun\n",
    "reg_chunker(raw, r'NP: {<NN.?>+<NN.?>}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP interactive/JJ agent/NN)\n",
      "(NP artificial/JJ intelligence/NN)\n",
      "(NP conversational/JJ partner/NN)\n",
      "(NP sophisticated/JJ natural/JJ language/NN)\n",
      "(NP many/JJ simpler/NN)\n",
      "(NP similar/JJ wording/NN)\n",
      "(NP conversational/JJ commerce/NN)\n"
     ]
    }
   ],
   "source": [
    "# Chunk 2: Adjective Follwed by Singular Noun\n",
    "reg_chunker(raw, r'NP: {<JJ>+<NN>}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the file connection like you're supposed to\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Clothing Reviews with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ClothingReviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Review Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_tag(text):\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['CHUNK'])\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        df = df.append({'CHUNK': chunk.text}, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our reviews to lowercase to simplify our search\n",
    "df[\"Review Text\"] = df[\"Review Text\"].str.lower()\n",
    "\n",
    "# Find only reviews that have the word 'dress' in them\n",
    "filter = df['Review Text'].str.contains('dress')\n",
    "df_dress = df[filter].copy()\n",
    "df_dress.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the results\n",
    "df_np = pd.DataFrame(columns = ['CHUNK'])\n",
    "\n",
    "# Iterate through the reviews and extra non-phrases for the reivews with \"small or little\"\n",
    "df_np = np_tag(df_dress['Review Text'].to_string())\n",
    "df_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 noun phrases - notice that there are a lot of filler words (stop words)\n",
    "df_np.groupby('CHUNK')['CHUNK'].count().\\\n",
    "    reset_index(name='count').sort_values(['count'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As opposed to removing stop words, we can filter out rows in the dataframe\n",
    "# that have the stop words.  This is a better way for noun phrases since we won't lose\n",
    "# the context of the phrases during our prior extraction. \n",
    "filter = (df_np['CHUNK'].str.contains('this')) | \\\n",
    "         (df_np['CHUNK'].str.contains('the')) | \\\n",
    "         (df_np['CHUNK'].str.contains('that')) | \\\n",
    "         (df_np['CHUNK'].str.contains('my')) | \\\n",
    "         (df_np['CHUNK'].str.contains('a')) | \\\n",
    "         (df_np['CHUNK'].str.len() < 6)\n",
    "df_np = df_np[-filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for words with spaces, so that we get only phrases with more than one word.\n",
    "filter = (df_np['CHUNK'].str.contains(' '))\n",
    "df_np = df_np[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np.groupby('CHUNK')['CHUNK'].count().\\\n",
    "    reset_index(name='count').sort_values(['count'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72a5e627de58a51027d2a5da874e1175d8953b8d9d561b504d2c65fac803dceb"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('EDA': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
