{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Apple is looking at buying U.K. startup for $1 billion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup VERB VBD dep xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(s)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please INTJ\n",
      "book VERB\n",
      "my PRON\n",
      "flight NOUN\n",
      "to ADP\n",
      "California PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Please book my flight to California\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "read VERB\n",
      "a DET\n",
      "very ADV\n",
      "good ADJ\n",
      "book NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I read a very good book\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Please', 'NNP'),\n",
       "  ('book', 'NN'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('flight', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('California', 'NNP')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent = sent_tokenize(\"Please book my flight to California\")\n",
    "\n",
    "[nltk.pos_tag(nltk.word_tokenize(word)) for word in tokenized_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I', 'PRP'),\n",
       "  ('read', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('very', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('book', 'NN')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent = sent_tokenize(\"I read a very good book\")\n",
    "\n",
    "[nltk.pos_tag(nltk.word_tokenize(word)) for word in tokenized_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "auto manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward auto manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "auto manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward auto manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ClothingReviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Department Name', 'Class Name', 'Review Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Title'] + ' ' + df['Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Title', 'Review Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column 'text_len' that counts the length for the derived field\n",
    "df['text_len'] = df.apply(lambda row: len(row['Text']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-crystal",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "For **Parts** of our analysis, the text needs to have some basic transformation for our models to work propertly.  These are as follows:\n",
    "\n",
    "1. **Lower**: Convert all characters to lowercase\n",
    "1. **Remove Punctuation**: In most cases, punctuation doesn't help NLP and ML models and can be removed.\n",
    "1. **Stop Word Removal**: Stop words generally don't add context to analysis (unless the length of text is very short (`100` - `200` characters) and can be removed.\n",
    "1. **Lemmatization**: Words will be reduced to there *Lemma* or root.  This will greatly improve the accuracy of the analysis since words like `simming` and `swimmer` will be reduced to `swim`.\n",
    "\n",
    "**Note**: The orginal text will be preserved for other analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text, stem=\"None\"):\n",
    "    \n",
    "    final_string = \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n",
    "    useless_words = useless_words + ['.', ',', '!', \"'\"]\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "    \n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "    \n",
    "    for word in text_stemmed:\n",
    "        final_string += word + \" \"\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_Processed'] = df['Text'].apply(lambda x: process_string(x, stem='Lem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_Processed'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-thirty",
   "metadata": {},
   "source": [
    "# Part-of-speech Tagging\n",
    "\n",
    "In this portion of the analysis, we will use a method known as Part of Speech tagging or POS.  This method uses a lexicon-based lookup to identify what parts of speech words are, such as Nouns, Verbs, Adjectives, and Adverbs.  By utilizing these, we can perform various analysis such as the following:\n",
    "\n",
    "- Word counts for different parts of speech to start giving us an overview of the most common parts of language used.\n",
    "- Identify nouns used to tag products (e.g., dress, jacket, bottom, etc.).  This will give us a different look at the popularity of items vs. simply counting categories.\n",
    "- Finally, we'll identify the top adjectives and adverbs for positive vs. negative reviews, telling us what words are used in each sentiment to describe the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "df['Text_Tok'] = df['Text_Processed'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(x):\n",
    "    '''using TextBlob, get the full parsed results (POS, etc)'''\n",
    "    blob = TextBlob(x)\n",
    "    p = blob.parse()\n",
    "    p = re.sub(r'^\\w+/', '',p)\n",
    "    return p.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pos(x):\n",
    "    '''pass a DataFrame column with tokenized text and return a DF of the Words'''\n",
    "    all_words = []\n",
    "    for l in x:\n",
    "        all_words = all_words + l\n",
    "        \n",
    "    df = pd.DataFrame(all_words)\n",
    "    df.columns = ['Word']\n",
    "    \n",
    "    # Add a column for the POS\n",
    "    df['Parse'] = df['Word'].apply(lambda x: parse_text(x))\n",
    "    \n",
    "    # Expned the extracted list of POS tags into their own columns, and concat that back to the orig DF\n",
    "    # https://chrisalbon.com/python/data_wrangling/pandas_expand_cells_containing_lists/\n",
    "    par = pd.DataFrame(df['Parse'].to_list(), columns=['P1','P2', 'P3', 'P4'])\n",
    "    df = pd.concat([df[:], par[:]], axis=1)\n",
    "    df.drop(columns=['Parse'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = build_pos(df['Text_Tok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-certification",
   "metadata": {},
   "source": [
    "**Notes:** Rather than using the much simpler approach of the POS with the TextBlog `tags` function[1], We used the `parse` function since it provides more verbose labeling text.\n",
    "\n",
    "The attempt here was to discover if there was a better way to identify nouns that would represent product features vs. other nouns.  Unfortunately, this didn't end up providing the detail needed. More information on this is presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-mitchell",
   "metadata": {},
   "source": [
    "## Word Counts for Different Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_pos = df_words.groupby('P1')['P1'].count().\\\n",
    "    reset_index(name='count').sort_values(['count'],ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data =df_top_pos, x='P1', y='count', palette=\"tab20\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-parallel",
   "metadata": {},
   "source": [
    "## Identify Top Product Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = df_words[df_words['P1'] == 'NN'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.groupby('Word')['Word'].count().reset_index(name='count').\\\n",
    "    sort_values(['count'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-orientation",
   "metadata": {},
   "source": [
    "**Notes**:  When inspecting `nouns` only, there is a mix of different types of words displayed, and some we can see are not tagged in such a way that seems to make sense with this dataset.  For example, `love` is tagged as a noun, but it's probably an adjective.  `bit` is probably referring to an adjective as well but is showing as a noun.\n",
    "\n",
    "We can inspect these words directly to see if there is a difference in their POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TextBlob('dress').parse())\n",
    "print(TextBlob('love').parse())\n",
    "print(TextBlob('bit').parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-michael",
   "metadata": {},
   "source": [
    "**Observations:**  When we try to use the Part of Speech (POS) tagging, there isn't a distinction between Nouns.  Each of these has the same POS sequence. \n",
    "\n",
    "We can use the Class name to determine clothing nouns to use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a list of all the unique class names\n",
    "noun_types = list(df['Class Name'].unique())\n",
    "\n",
    "# The words from the categories need to be lemmatized.\n",
    "lem = WordNetLemmatizer()\n",
    "for i in range(len(noun_types)):\n",
    "    noun_types[i] = lem.lemmatize(noun_types[i].lower())\n",
    "noun_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the text into a huge string and use Text Blobs to get a Dictionary out with counts\n",
    "all_text = ' '.join(df['Text_Processed'])\n",
    "all_text_blob = TextBlob(all_text)\n",
    "all_text_dict = all_text_blob.word_counts\n",
    "\n",
    "# Turn the dictionary into a Dataframe.  Filter by the word list and then sort for plotting.\n",
    "df_dict = pd.DataFrame(list(all_text_dict.items()),columns = ['Word','Count']) \n",
    "df_products = df_dict[df_dict.Word.isin(noun_types)]\n",
    "df_products.sort_values(by=['Count'], inplace=True, ascending=False)\n",
    "df_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-potential",
   "metadata": {},
   "source": [
    "**Observations**: Based on the top outputs, we can see that `dresses` are the largest mentioned product line at a rate of `4x` the second, `sweaters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.barplot(x='Word', y='Count', data=df_products, palette=\"tab20\", dodge=False)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72a5e627de58a51027d2a5da874e1175d8953b8d9d561b504d2c65fac803dceb"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('EDA': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
